{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SolomonM-Kebede/Transformer-Based-NLP/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AMDZ8M1gY31C"
      },
      "source": [
        "# Transformer-based Natural Language Processing\n",
        "## Introduction to ðŸ¤— Transformers\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe23-TFb-NLP/blob/master/assignment.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bpKcq6ApY31E"
      },
      "source": [
        "### Installing necessary packages (i.e. if on Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "eIbBLEpAY31E",
        "outputId": "e48c19ff-13bb-4466-b718-28dd80fb2fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Colab:\n",
        "!pip install torch datasets tokenizers transformers\n",
        "\n",
        "# Other:\n",
        "# % pip install torch datasets tokenizers transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lzfTBcV9Y31F"
      },
      "source": [
        "### Premise\n",
        "\n",
        "This notebook will guide you through the process of finetuning a transformer model using the [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index) library.\n",
        "\n",
        "First, we need to select a task and suitable dataset. Here, we will use the [Textual Entailment or Natrual Language Inference](https://cims.nyu.edu/~sbowman/multinli/) task as an example. A suitable dataset can be found in the [GLUE repository on the ðŸ¤— Hub](https://huggingface.co/datasets/glue). The whole MNLI dataset ist way too big, so we will only use a slice of it.\n",
        "\n",
        "We can load the MNLI (slice) of the GLUE dataset using [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "a_W1Rrx1Y31F",
        "outputId": "3fcf53d3-24f7-44be-f9dc-c2664e7e0931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
            "        num_rows: 39270\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
            "        num_rows: 9815\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
            "        num_rows: 9796\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "mnli_dataset = load_dataset(\"glue\", \"mnli\", split={\"train\": \"train[:10%]\", \"validation\": \"validation_matched\", \"test\": \"test_matched\"})\n",
        "print(mnli_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "UUE-_9sKY31F"
      },
      "source": [
        "As we can see above, the dataset is already split into train, development and test splits.\n",
        "Each row contains four, but we only need to focus the premise, hypothesis and the label.\n",
        "\n",
        "The textual entailment task requires us to recognize, given two text fragments, whether the meaning of one text is entailed (*can be inferred*) from the other text.\n",
        "\n",
        "In this example, we will use a BERT-family model. With BERT, we formulate the entailment task as a simple classification task by concatenating the premise and hypothesis and training our classifier on the first token (the `[CLS]` token) of the input string:\n",
        "\n",
        "```\n",
        "\"[CLS] This is the premise, i.e. a text that means something. [SEP] This is the hypothesis, i.e. what we may be able to infer [SEP]\"\n",
        "```\n",
        "\n",
        "But let's first take a look at the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "EkJ1DJO6Y31F",
        "outputId": "22cf3717-da3e-4bcc-aeb1-c636a4bdbe55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.', 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him'], 'hypothesis': ['Product and geography are what make cream skimming work. ', 'You lose the things to the following level if the people recall.'], 'label': [1, 0], 'idx': [0, 1]}\n",
            "{'premise': ['The new rights are nice enough', 'This site includes a list of all award winners and a searchable database of Government Executive articles.'], 'hypothesis': ['Everyone really likes the newest benefits ', 'The Government Executive articles housed on the website are not able to be searched.'], 'label': [1, 2], 'idx': [0, 1]}\n",
            "{'premise': ['Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out for.', 'The extent of the behavioral effects would depend in part on the structure of the individual account program and any limits on accessing the funds.'], 'hypothesis': ['Hierbas is a name worth looking out for.', 'Many people would be very unhappy to loose control over their own money.'], 'label': [-1, -1], 'idx': [0, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(mnli_dataset['train'][:2])\n",
        "print(mnli_dataset['validation'][:2])\n",
        "print(mnli_dataset['test'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "aROsvXHpY31G"
      },
      "source": [
        "As we see above, the `test_matched` split contains **unlabeled** samples, be we can ignore that for now.\n",
        "\n",
        "Let's construct the sentences as we outlined above.\n",
        "\n",
        "*Note:* The `[CLS]` and final `[SEP]` will be added by the BERT's tokenizer, so we omit them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "MYfZbshCY31G",
        "outputId": "c05da8dd-0088-4e18-af2d-65ae42ba04f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'label': [1, 0], 'input': ['Conceptually cream skimming has two basic dimensions - product and geography. [SEP] Product and geography are what make cream skimming work. ', 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him [SEP] You lose the things to the following level if the people recall.']}\n"
          ]
        }
      ],
      "source": [
        "prepared_dataset = mnli_dataset.map(\n",
        "    lambda sample: {'input': f\"{sample['premise']} [SEP] {sample['hypothesis']}\"},\n",
        "    remove_columns=['premise', 'hypothesis', 'idx']\n",
        ")\n",
        "print(prepared_dataset['train'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Maj6WwXWY31G"
      },
      "source": [
        "*Hint:* It is also possible to use the BERT tokenizer directly to construct the samples as shown above, skipping this preparation step entirely!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1SmxxG-AY31G"
      },
      "source": [
        "### Loading Pre-Trained Models\n",
        "\n",
        "Now we need to load a pre-trained [BERT](https://github.com/google-research/bert) model. You should use a subclass of [AutoModel](https://huggingface.co/docs/transformers/main/en/autoclass_tutorial).\n",
        "\n",
        "Viable pre-trained BERT models include:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "    <th>Model</th><th>Reference</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><a href=\"https://huggingface.co/bert-base-uncased\">bert-base-uncased</a></td>\n",
        "    <td rowspan=\"6\"><a href=\"https://aclanthology.org/N19-1423/\">Devlin et al., 2019</a></td>\n",
        "</tr>\n",
        "<tr><td><a href=\"https://huggingface.co/bert-base-cased\">bert-base-cased</a></td></tr>\n",
        "<tr><td><a href=\"https://huggingface.co/bert-large-uncased\">bert-large-uncased</a></td></tr>\n",
        "<tr><td><a href=\"https://huggingface.co/bert-large-cased\">bert-large-cased</a></td></tr>\n",
        "<tr><td><a href=\"https://huggingface.co/bert-large-uncased-whole-word-masking\">bert-large-uncased-whole-word-masking</a></td></tr>\n",
        "<tr><td><a href=\"https://huggingface.co/bert-large-cased-whole-word-masking\">bert-large-cased-whole-word-masking</a></td></tr>\n",
        "<tr><td colspan=\"2\"></td></tr>\n",
        "<tr>\n",
        "    <td><a href=\"https://huggingface.co/prajjwal1/bert-tiny\">prajjwal1/bert-tiny</a></td>\n",
        "    <td rowspan=\"4\"><a href=\"https://arxiv.org/abs/1908.08962\">Turc et al., 2019</a>; <a href=\"https://arxiv.org/abs/2110.01518\">Bhargava et al., 2021</a></td>\n",
        "</tr>\n",
        "<tr><td><a href=\"https://huggingface.co/prajjwal1/bert-mini\">prajjwal1/bert-mini</a></td></tr>\n",
        "<tr><td><a href=\"https://huggingface.co/prajjwal1/bert-small\">prajjwal1/bert-small</a></td></tr>\n",
        "<tr><td><a href=\"https://huggingface.co/prajjwal1/bert-medium\">prajjwal1/bert-medium</a></td></tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "#### Load and instantiate a model for the textual entailment task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3_2gw04TY31G"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer  #, AutoModelFor?\n",
        "\n",
        "config = ...  # TODO\n",
        "tokenizer = ...  # TODO\n",
        "model = ...  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1huW7-GEY31G"
      },
      "source": [
        "Now we could use the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for easy training. You can follow the tutorial from [the official documentation](https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop).\n",
        "\n",
        "#### Write the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "YVD-m-huY31G"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(...)\n",
        "\n",
        "# TODO\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# TODO: evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "XrKTkbF7Y31G"
      },
      "source": [
        "### Custom Training\n",
        "While using the trainer class is very convenient, if you have to run custom procedures during training, a regular training loop can be more accessible.\n",
        "\n",
        "We do need to do our own tokenization, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IrFNqK0hY31G"
      },
      "outputs": [],
      "source": [
        "def encode_pt(batch: dict):\n",
        "    return tokenizer(\n",
        "        batch['input'],\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=False,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "\n",
        "pt_dataset = prepared_dataset.map(encode_pt)\n",
        "print(pt_dataset['train'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PJCXDjl4Y31G"
      },
      "source": [
        "However, in a manual training loop, we will want to make use of PyTorch's DataLoaders, which require some extra care to collate batches with samples of different lengths.\n",
        "\n",
        "#### Implement `custom_collate`:\n",
        "- Pad and stack the `input_ids` in a tensor.\n",
        "- Stack the labels in a tensor of type `long`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "jiLI_JOBY31H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def custom_collate(batch: list[dict]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    input_ids = ...\n",
        "    label = ...\n",
        "    return input_ids, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Z74T_5deY31H"
      },
      "source": [
        "#### Write the training and evaluation loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "LHsyLntsY31H"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm, trange\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import *  # TODO\n",
        "from torch.nn import *  # TODO\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "criterion = ...  # TODO\n",
        "optimizer = ...  # TODO\n",
        "num_epochs = ...  # TODO\n",
        "batch_size = ...  # TODO\n",
        "\n",
        "train_dataloader = DataLoader(pt_dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
        "dev_dataloader = DataLoader(pt_dataset['validation'], batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "model.to(device)\n",
        "for epoch in trange(num_epochs, position=0):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader, position=1, leave=False):\n",
        "        ...  # TODO\n",
        "\n",
        "    model.eval()\n",
        "    for batch in tqdm(dev_dataloader, position=1, leave=False):\n",
        "        ...  # TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}